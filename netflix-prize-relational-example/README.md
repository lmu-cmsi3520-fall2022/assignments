**CMSI 3520** Database Systems, Fall 2022

# Netflix Prize Relational Database Mini-Stack Example
This folder and README contains sample commands and code that correspond to what is being requested for the [Relational Database Mini-Stack assignment](../README.md). To save repository space, the data files themselves are _not_ included here: before trying out these commands and programs, please [download the files](https://www.kaggle.com/netflix-inc/netflix-prize-data) into this folder first.

To avoid accidental committing of these files, a _.gitignore_ file has been placed here as well. We are definitely not taking an extra two gigabytes of repository space lightly!

## The Greater Schema of Things
The first step in implementing a relational database is to determine its _schema_ (or _logical schema_, if taken in the full context of how these systems are structured). This is done by studying the dataset first in order to get to know its _canonical schema_‚Äîi.e., its overall structure independent of the database that‚Äôs implementing it. Then, with that idea in your head, the structure then needs to be rendered in terms of tables, columns, and keys, as initially described in the [Super Basic SQL](https://dondi.lmu.build/share/db/super-basic-sql.pdf) document.

After studying the Netflix Prize data, this leads to a relational database diagram ([schema.pdf](./schema.pdf)) and corresponding SQL statements that implement this diagram in PostgreSQL ([schema.sql](./schema.sql)). You might recall that [DB Fiddle](https://www.db-fiddle.com) refers to this as the ‚ÄúSchema SQL.‚Äù More formally, this is also known as the _DDL_ (database definition language) portion of SQL‚Äîthe subset of SQL whose role is to describe the database that we‚Äôre building.

The database diagram for this assignment should follow the notation previously given in the [Super Basic Database Diagramming](https://dondi.lmu.build/share/db/super-basic-database-diagramming.pdf) document.

### Differences from the DB Fiddle Schema
Speaking of that document, you‚Äôll notice that the [schema.pdf](./schema.pdf) included here is similar to it but not identical. Mainly, our schema no longer has a _viewer_ table. Ideally, we would, but because we will now be working with the dataset as given, we can‚Äôt really justify having a _viewer_ table because there is no further information provided about the viewer. So we will just take the viewer IDs as values in their own right which don‚Äôt reference any other table.

For [schema.sql](./schema.sql) there is also something new that wasn‚Äôt seen in the corresponding [Netflix DB Fiddle example](https://www.db-fiddle.com/f/o2ohcGVAgHZQg4teg1s9jW/6): the `id` primary key of the _movie_ table has the additional qualifier `GENERATED BY DEFAULT AS IDENTITY`. [`GENERATED BY DEFAULT AS IDENTITY` (search for ‚Äúas identity‚Äù at this link)](https://www.postgresql.org/docs/current/sql-createtable.html) specifies that a column will auto-generate a new value based on an internally-managed sequence (you can type `\ds` into _psql_ to see that sequence if you‚Äôre interested). We‚Äôll use it here so that when new movies are added to the database, we only need to specify `year` and `title`‚Äîthey will automatically get a valid `id`.

## Set the Table(s)
Once your database has been planned out, it‚Äôs time to actually set it up in a running database server. Assuming that you have gotten a [PostgreSQL server up and running](http://dondi.lmu.build/share/db/postgresql-setup-day.pdf), you can pretty much feed your schema file to PostgreSQL via _psql_:

    psql -f schema.sql postgresql://localhost/postgres

The example above uses the `-f` option of _psql_ to send a file containing SQL statements directly into the database.

Of course, you can also just run _psql_ and type the `CREATE TABLE` statements directly at first‚Äîin fact, when just starting out, you might prefer to go this route first because it may take a few iterations to get your DDL exactly right.

### `DROP TABLE`, `ALTER TABLE`, and `\d`
Some useful new options while you‚Äôre in this phase include:
* The [SQL `DROP TABLE` statement](https://www.postgresql.org/docs/current/sql-droptable.html) removes a table from the database in case you need to create it again
* [`ALTER TABLE` is an SQL alternative](https://www.postgresql.org/docs/current/sql-altertable.html) that allows you to just change an existing table rather than removing it and creating it over‚Äîimportant if your table already has data in it! But if you do this on the fly, make sure to still capture the latest changes in your _schema.sql_ so that if you decide to start over, you‚Äôll get the latest version of your table definition
* The _psql_ `\dt` command will show you the list of current tables in your database
* Also available in _psql_ is `\d <table name>`, which will show you the definition of a given table

Don‚Äôt forget that you can use `\h` in _psql_ to get online help on any SQL statement and `\?` will provide help on the non-SQL utility commands.

## Put the ‚ÄúData‚Äù in ‚ÄúDatabase‚Äù
Technically, once your tables are defined your database is ready to go. But‚Ä¶it‚Äôs empty. You can use it but you can‚Äôt do much with it until you give it some data. This is where your dataset‚Äôs files come in‚Äîit‚Äôs time to get their information into those tables.

There are many ways to get data into a relational database in general and PostgreSQL in particular. For this case study, we will show two alternative approaches: one that is highly portable and another that is more suitable to bulk imports but a little less flexible.

### `INSERT`: Portable but Slow
First, the portable approach: feeding `INSERT` statements into _psql_. It‚Äôs highly portable because the approach is independent of any particular programming language, utility, or library‚Äîit doesn‚Äôt matter how you generate the `INSERT` statements; it just matters that you _do_ generate them. We will use this approach with _movie_titles.csv_ since that file isn‚Äôt very large.

The [_movie_loader.py_](./movie_loader.py) program will read the _movie_titles.csv_ file and print `INSERT` statements corresponding to each movie record in those files.

### `COPY`: Proprietary but Fast
Due to the size of the _combined_data*.txt_ files, we will use a more efficient approach with these: this is the PostgreSQL-specific [`COPY` statement](https://www.postgresql.org/docs/current/sql-copy.html). Right there you can see why this isn‚Äôt portable‚Äî`COPY` is not standard SQL. But it will save a ton of time for very large datasets.

The `COPY` statement expects data to be provided in a specific format: by default, this consists of tab-delimited lines, one line per row, with columns separated by tab characters.

The [_rating_loader.py_](./rating_loader.py) program will read the _combined_data*.txt_ files and issue the `COPY` statement followed by the formatted data. The data stream ends with a line containing backslash-period (`\.`), which is `COPY`‚Äôs designated ‚Äúend of data‚Äù signal. You‚Äôll notice that _rating_loader.py_ uses similar logic to our previous preprocessor program by watching out for those movie ID lines (the ones with just a movie ID followed by a colon) and making sure that the current movie ID is included in each emitted data line.

How much of a difference does `COPY` make, you might ask? See for yourself‚Äîthe [_rating_insert_loader.py_](./rating_insert_loader.py) program uses the more compatible `INSERT` approach for the ratings data. Run it the way you would run _movie_loader.py_ (see below) and see how long it takes. (_Hint:_ Unless your machine is hugely fast, chances are you won‚Äôt want to wait for it to finish ü§®)

### Run, PostgreSQL, Run
When run by themselves, the `INSERT` statements or `COPY` + data lines are simply printed out. Once you are satisfied that the statements are indeed correct‚Äîi.e., they will correctly add the data from the files into the database‚Äîyou can then use the handy `|` directive to send those printed commands into _psql_:

    python3 movie_loader.py | psql postgresql://localhost/postgres
    python3 rating_loader.py | psql postgresql://localhost/postgres

Alternatively, you can write the output of those programs into ‚ÄúSQL script‚Äù files then make _psql_ execute those files:

    python3 movie_loader.py > movie_data.sql
    psql postgresql://localhost/postgres -f movie_data.sql
    python3 rating_loader.py > rating_data.sql
    psql postgresql://localhost/postgres -f rating_data.sql

### Caveat Encoder
The programs included here assume that you are operating in an all-UTF-8 environment‚Äîi.e., a system where UTF-8 encoding (remember that?) is the standard or default way to encode characters. The `initdb` template that you‚Äôve been given also ensures that the database cluster you create is UTF-8-savvy.

If you happen to be in an environment that does _not_ handle UTF-8 seamlessly, the import process may encounter errors. If the error messages mention unrecognized or illegal characters, then you are likely working in a system where one or more parts can‚Äôt handle UTF-8.

Exact solutions vary depending on the part of your setup that can‚Äôt handle UTF-8, but good candidates include:

* Add this line to a loader program before it prints anything:

    ```python
    sys.stdout.reconfigure(encoding='utf-8')
    ```

    This will make Python insist on printing output as UTF-8 vs. just following the encoding of the command line shell

* The most likely non-UTF-8-savvy parts of current machines are the Windows terminal programs Command Prompt and PowerShell (Git Bash and Ubuntu on Windows are OK, last I checked). In this case, one can _force_ these programs to use UTF-8‚Äîinstructions can be found here: https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window

### `DELETE`
As with schema definition, you might need to iterate through this when you‚Äôre writing your own loaders, potentially resulting in leftover data that would get duplicated if you ran your loader again. To assist with this, you may either use `DROP TABLE` to remove a table entirely‚Äîmeaning you have to invoke `CREATE TABLE` again‚Äîor you can instead use the `DELETE` statement. The super-concise (and super-dangerous!) `DELETE FROM <table>;` will unconditionally remove every row in that table. Use it with caution! To be more precise about what to delete, you can add a `WHERE` clause to the `DELETE` statement: it works just like the `WHERE` clause in `SELECT`, except that matching rows are _removed_ from the table rather than returned as a result.

### A Note About Scale
The Netflix Prize dataset consists of around 17,700 movies‚Äîfairly small as real datasets go. But note there are _more than **100,000,000** ratings_ in the dataset taking up more than 2 gigabytes of data‚Äîthat‚Äôs the real deal! This scale means that:
* Converting to `COPY` + data lines or `INSERT` statements means you‚Äôll produce more than 100 million lines‚Äîpre-writing these commands out will produce a very large file!
* `COPY` is built for speed, but executing 100 million `INSERT` statements into _psql_ will definitely take some time‚Äîon the order of _hours_ on current hardware, possibly _days_

Thus, when launching into this step of the case study‚Ä¶
* Make sure that your computer has enough storage space for 100 million ratings records
* If you want to try out the `INSERT` approach for something this huge, start the ratings load at a time when you can leave your computer running for many many hours‚Äîovernight (or more) would be ideal

We definitely aren‚Äôt fooling around here üßê ‚Äîbut the hope is that the scale is worth it, because it will help you truly appreciate what generalized database management systems do for us. Imagine writing programs that do a sequential scan of these ratings _every single time_ you want to retrieve something from this dataset. Or better yet, just don‚Äôt imagine it and skip right to using a database üòé

In case we haven‚Äôt made it clear enough yet: it will take **multiple gigabytes** to fully load PostgreSQL with the full ratings data. So plan accordingly.

The good news is that, while lengthy processes may be running on one terminal window, you can always connect to the database concurrently on another window. So you can definitely start issuing queries and practicing things even while database loading is going on‚Äîyou merely won‚Äôt have all of the data at your fingertips yet until all of the records have landed.

## Time for Some Play<i>SQ</i>oo<i>L</i>
Once the database has some data in it (doesn‚Äôt have to be completely full yet, as mentioned previously), you can start honing your SQL skills. You can start with the level of SQL shown in the [Super Basic SQL crib sheet](http://dondi.lmu.build/share/db/super-basic-sql.pdf) and the original [Netflix DB Fiddle example](https://www.db-fiddle.com/f/o2ohcGVAgHZQg4teg1s9jW/6) (minus references to the _viewer_ table)‚Äîjust think of a question in plain English and see if you can translate that into a corresponding [`SELECT` statement](https://www.postgresql.org/docs/current/sql-select.html). The PostgreSQL documentation also provides a [querying tutorial](https://www.postgresql.org/docs/current/tutorial-select.html) if you want to see additional examples at a basic level.

Issuing `SELECT` statements is a matter of running _psql_ on your database server then typing the statement directly at the _psql_ prompt. Make sure to terminate your statements with a semicolon `;` so that _psql_ knows when you‚Äôre done.

* List all columns for movies containing both `'and'` and `'of'` in their titles, sorted ascending by year then title:
```
postgres=# SELECT * FROM movie WHERE title LIKE '% and %' AND title LIKE '% of %' ORDER BY year, title;
  id   | year |                                title                                 
-------+------+----------------------------------------------------------------------
  3790 | 1921 | Avant-Garde: Experimental Cinema of the 1920s and '30s
  2449 | 1939 | The Private Lives of Elizabeth and Essex
 12416 | 1942 | Sherlock Holmes and the Voice of Terror
  7220 | 1946 | Anna and the King of Siam
 17715 | 1947 | The Adventures of Ma and Pa Kettle: Vol. 1
   ...
   ...
(96 rows)
```

* List movie titles released in the 20th century with the substring `'future'` in their titles, case-insensitively, sorted ascending by title:
```
postgres=# SELECT title FROM movie WHERE year < 2001 AND title ILIKE '%future%' ORDER BY title;
                title                 
--------------------------------------
 Back to the Future
 Back to the Future Part II
 Back to the Future Part III
 Future War
 Futuresport
 Ivan Vasilievich: Back to the Future
 Megaman: Battle for the Future
 The X-Files: Fight the Future
(8 rows)
```

Due to the size of the _rating_ table, be prepared for some of these queries to take a few moments (but definitely not hours):

* Determine the average rating given by all viewers on the month of October, 2004:
```
postgres=# SELECT AVG(rating) FROM rating WHERE date_rated >= '2004-10-01' AND date_rated <= '2004-10-31';
        avg         
--------------------
 3.6811938432580795
(1 row)
```

* List the titles of movies that got a rating of 1 on December 25, 2002, sorted ascending by title:
```
postgres=# SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
                                    title                                    
-----------------------------------------------------------------------------
 102 Dalmatians
 13 Conversations About One Thing
 3000 Miles to Graceland
 A Couch in New York
 A Fish Called Wanda
   ...
   ...
(429 rows)
```

* List the year and rating count for movies released before 1910 or after 2000, sorted descending by year:
```
postgres=# SELECT year, count(*) FROM movie, rating WHERE movie.id = rating.movie_id AND (year < 1910 OR year > 2000) GROUP BY year ORDER BY year DESC;
 year |  count   
------+----------
 2005 |  1983802
 2004 | 10456339
 2003 |  9576604
 2002 |  8640932
 2001 |  7241888
 1909 |      109
 1896 |      152
(7 rows)
```

The possibilities go on and on‚Ä¶the fun never stops!

### A Word About Correctness
When you‚Äôre still starting out with SQL and you have this much data, it will be good to know whether the query you formulated really did return the results you were looking for. For some queries, this will be self-evident‚Äîyou can check from the movie titles, years, or other attributes whether the returned records meet your specified conditions. However, some queries are a little harder to check. For example, with the query given at the end of the previous section, how would you know whether the counts were correct?

Although the very large results may be harder to tally, for the smaller counts, one can potentially formulate a different but related query then see whether those results align with the original query‚Äôs results. Thus, one can potentially retrieve all ratings given to movies released in 1896 to see whether there are indeed 152 of them:

```
postgres=# SELECT * FROM movie, rating WHERE movie.id = rating.movie_id AND year = 1896;
  id  | year |             title             | movie_id | viewer_id | rating | date_rated 
------+------+-------------------------------+----------+-----------+--------+------------
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   2482518 |      5 | 2004-09-24
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |    440294 |      5 | 2004-03-05
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   1062414 |      4 | 2004-06-03
 7654 | 1896 | Lumiere Brothers' First Films |     7654 |   2092745 |      5 | 2005-04-07
   ...
   ...
(152 rows)
```

After paging through all of the results, one does get the final count message of `(152 rows)` so this helps to assure us that our earlier, more complex query did indeed give us the result that we wanted.

### Learning/Using Other Comparators
Take note that PostgreSQL is capable of a wide range of comparators for strings and other data types‚Äîsome are part of the universal SQL standard and some are specific to PostgreSQL‚Äôs _dialect_ of SQL. It isn‚Äôt necessary to know each of these variations off the top of your head but it‚Äôs good to know that they‚Äôre at there and also good to know how to look them up when needed:
- [Standard comparators](https://www.postgresql.org/docs/current/functions-comparison.html)
- [String pattern matching](https://www.postgresql.org/docs/current/functions-matching.html)
- [Date/time comparators](https://www.postgresql.org/docs/current/functions-datetime.html)

The full range of capabilities can be found in the [Functions and Operators](https://www.postgresql.org/docs/current/functions.html) section of the PostgreSQL documentation. It‚Äôs actually quite mind-boggling‚Äîjust take a peek and walk away with what seems immediately useful or interesting. Plan on looking things up later as needed.

### Using `LIMIT`
For some information needs, you don‚Äôt need to know _all_ of the results at once‚Äîyou might want just a subset. That‚Äôs where `LIMIT` comes in. Adding a `LIMIT` to a `SELECT` statement, followed by some number _n_, will only return a maximum of _n_ matching results. When accompanied with the correct `ORDER BY` clause, this allows you to issue top-_n_ queries. In the broader context of web services, `LIMIT` facilitates _pagination_, where users only see a maximum number of records at a time and need to _page_ forward or backward through the results in distinct steps.

* List the IDs and average ratings of the top 5 movies in terms of average rating, sorted descending by average rating then by movie ID in case of a tie:
```
postgres=# SELECT movie_id, AVG(rating) FROM rating GROUP BY movie_id ORDER BY AVG(rating) DESC, movie_id LIMIT 5;
 movie_id |        avg         
----------+--------------------
    14961 | 4.7232699256835072
     7230 | 4.7166108250932963
     7057 | 4.7026110636480137
     3456 | 4.6709891019450959
     9864 | 4.6388093875214654
(5 rows)
```

For full details, [this section](https://www.postgresql.org/docs/current/queries-limit.html) of the PostgreSQL documentation describes the `LIMIT` keyword (along with `OFFSET`, which we won‚Äôt address here but should be easy to pick up upon reading that page).

### More Explicit Relationships with `INNER JOIN`
You have seen so far that we can connect multiple tables by naming them in the `FROM` clause of a `SELECT` statement then specifying a condition in the `WHERE` clause that determines how rows from one table are ‚Äúmatched up‚Äù with another:

```sql
SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
```

This works perfectly well but there is an alternate form for specifying this matchup which more explicitly states that a relationship exists between two tables: this form is the `INNER JOIN` expression. Using `INNER JOIN` places the related tables and the condition that connects them closer together, all in the `FROM` clause. This makes the intent of the query somewhat clearer and opens up the possibility for other kinds of `JOIN`s which we will see later:

```sql
SELECT DISTINCT title FROM movie INNER JOIN rating ON movie.id = rating.movie_id WHERE rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
```

When using the `INNER JOIN` expression, the two tables are named before and after `INNER JOIN` (thus making `INNER JOIN` feel like a binary operation, which it is) and an `ON` clause states the condition that connects the records of one table to the other. This moves the condition outside of the `WHERE` clause, thus more cleanly separating which conditions establish relationships vs. which conditions are meant to narrow down the data being requested.

Refer to [this tutorial](https://www.postgresql.org/docs/current/tutorial-join.html) for additional discussion of `JOIN`. A more comprehensive description of `JOIN` is provided in the [reference page for the `SELECT` statement](https://www.postgresql.org/docs/current/sql-select.html).

### The `HAVING` Clause: `WHERE` for `GROUP BY`
As you practice using SQL, you will eventually notice that there‚Äôs something that the `WHERE` clause appears to unintuitively reject: conditions on aggregate functions like `COUNT`, `AVG`, `MAX`, and `MIN`. For example, this query, which tries to determine movie IDs with a low average rating, generates an error:

```
postgres=# SELECT movie_id, AVG(rating) FROM rating WHERE AVG(rating) < 2 GROUP BY movie_id;
ERROR:  aggregate functions are not allowed in WHERE
LINE 1: SELECT movie_id, AVG(rating) FROM rating WHERE AVG(rating) <...
```

Conditions that involve these aggregate functions are disallowed from `WHERE` because `WHERE` is supposed to operate on _individual rows_ and not _groups of rows_, which is what aggregate functions use. Instead, a _separate_ clause called `HAVING` is used for conditions that involve these groups, and `HAVING` appears after `GROUP BY` to drive this point home:

```
postgres=# SELECT movie_id, AVG(rating) FROM rating GROUP BY movie_id HAVING AVG(rating) < 2;
 movie_id |        avg         
----------+--------------------
       41 | 1.6344086021505376
       53 | 1.6754385964912281
       86 | 1.9545454545454545
      151 | 1.9545454545454545
      415 | 1.7619047619047619
   ...
   ...
(313 rows)
```

One way to remember whether a condition goes in `WHERE` vs. `HAVING` is to ask yourself: ‚Äúif I‚Äôm looking at a _single row_ from one or more tables, do I have enough information to answer whether a condition is fulfilled?‚Äù If the answer is ‚Äúyes,‚Äù then you are looking at a `WHERE` condition. If the answer is ‚Äúno‚Äù‚Äîfor example, you can‚Äôt tell if a single row will fulfill some `COUNT` or `AVG` condition because you need multiple rows to calculate those in the first place‚Äîthen that condition belongs in `HAVING`.

### Making PostgreSQL `EXPLAIN` Itself
By way of a preview to future discussions, one last fun thing you can do with _psql_ is the `EXPLAIN` statement. `EXPLAIN` is simple: if you prepend `EXPLAIN` to any `SELECT` statement, instead of performing the query PostgreSQL will show _how_ it will perform the query. How many nested loops will there be? What operations will be brute-force scan vs. a different kind of algorithm? How much data will be covered. The availability of `EXPLAIN` is a great way to take a peek into the inner workings of the database system:

```
postgres=# EXPLAIN SELECT DISTINCT title FROM movie, rating WHERE movie.id = rating.movie_id AND rating = 1 AND date_rated = '2002-12-25' ORDER BY title;
                                          QUERY PLAN                                           
-----------------------------------------------------------------------------------------------
 Unique  (cost=1174410.13..1174420.18 rows=2011 width=22)
   ->  Sort  (cost=1174410.13..1174415.15 rows=2011 width=22)
         Sort Key: movie.title
         ->  Nested Loop  (cost=1000.29..1174299.79 rows=2011 width=22)
               ->  Gather  (cost=1000.00..1172342.43 rows=2011 width=4)
                     Workers Planned: 2
                     ->  Parallel Seq Scan on rating  (cost=0.00..1171141.33 rows=838 width=4)
                           Filter: ((rating = 1) AND (date_rated = '2002-12-25'::date))
               ->  Index Scan using movie_pkey on movie  (cost=0.29..0.97 rows=1 width=26)
                     Index Cond: (id = rating.movie_id)
(10 rows)
```

## Hello DAL-ly
As flexible and powerful as SQL is‚Äîor any database definition/manipulation language in general‚Äîthis isn‚Äôt how most people interact with database management systems. Take yourself, for example: you have probably been interacting with databases your whole life, but with a few exceptions, this is probably the first time you‚Äôre encountering SQL in depth.

Instead, most database interaction takes place through _applications_‚Äîuser-facing programs that have a distinct real-world purpose which happen to need a database management system in order to fulfill that purpose. These applications are no different from the programs and apps that you have already written to this point‚Äîthey just happen to talk to a database as part of their functionality. And, like with all code, they do that by importing some module and library that abstracts these CRUD operations out for them. This part of the software is called the _data access layer_, or DAL for short.

The DAL is just like any other library: it defines a range of functions that interact with a database in whatever ways the application needs. From an application‚Äôs perspective, it simply calls functions to create, read, update, or delete information from the database. The DAL‚Äôs _implementation_ takes care of the details.

For our case study, we provide the beginnings of a DAL in two versions: one written in Python and another written in Node.js. These can serve both as concrete examples of what a DAL is in the first place as well as potential starting points for writing your own DALs for your own datasets. Both DALs rely on a _general-purpose_ database library which makes the full functionality of the underlying database management system available; the job of the DAL is to _focus_ this functionality so that the application can go straight to accomplishing what it needs.

Of course, these aren‚Äôt the only platforms for which you can write DALs, either. If there‚Äôs a generalized database library for a language, then you can write a DAL in that language.

### Dawn of the DAL
It should be noted that the very notion of a distinct, separable/reusable module which isolates data access details from the rest of an application is a key idea in and of itself. In his seminal [‚ÄúA relational model of data for large shared data banks.‚Äù](https://dl.acm.org/doi/10.1145/362384.362685), E. F. Codd notes a need for _data independence_ in applications of the time. In his words, ‚Äúvariety of data representation characteristics which can be changed _without logically impairing some application programs_ is still quite limited‚Äù‚Äîitalics are his. It was in fact the very objective of increasing the independence of an application‚Äôs code from the internal details of a data store‚Äôs information that serves a key motivating factor behind his proposed relational database model.

Ultimately, the idea of a DAL is orthogonal to the relational database model as well, and we will see this idea integrated into all of our database mini-stacks. Its history is mentioned here not only because it‚Äôs the first mini-stack we are encountering but also to point out that sometimes, notions that might seem to be an obvious matter-of-course in the present day could very well have been missing or considered revolutionary in the past, until some particularly insightful yet straightforward-thinking innovator actually realized the need for such a thing and found a way to implement it feasibly.

### A Spectrum of DAL-liances: Relational DAL Implementation Styles
The relational database model fosters a range of styles when it comes to implementing DALs that target such databases. This range is illustrated in the following table, with some notes on the characteristics that typify certain approaches. Note that relational database libraries may offer any or all of these styles, with possible gradations in between. Feel free to pick the style that suits the task, on a function-by-function basis.

| | Raw/Embedded SQL | Structured/Built/‚ÄúFluent‚Äù SQL | Object-Relational Mapping (ORM) |
| --- | --- | --- | --- |
| _Description_ | SQL statements given as strings, yielding arrays or tuples varying types | SQL statements that are ‚Äúbuilt‚Äù through functions and data structures, yielding either varying arrays or mapped to a predefined data structure | Object definitions and methods that generate SQL automatically, returning instances of those objects |
| _Programming style_ | Build SQL strings on your own, ‚Äúsend‚Äù them through a function, and be ready to parse/interpret the results depending on the kind of query that was sent | Use functions to build an object that represent an SQL operation, ‚Äúinvoke‚Äù that object, then parse/interpret the results depending on how the return value was specified | Determine the objects of interest in advance, then call methods/functions on those objects that return instances of those objects‚ÄîSQL is generated under the hood |
| _What You Gain‚Ä¶_ | Total flexibility: If you can say it in SQL, you can invoke it | Error avoidance: SQL builder functions are structured in a way that will prevent syntactically malformed SQL | Domain-specificity: Express your data needs in terms of the application‚Äôs objects and structures‚Äîproperty names and types are known and many operations (especially joins) are done automatically and intuitively |
| _‚Ä¶in Exchange For_ | Send any string, you say? Yes‚Äîwhich means you better build a _correct_ one! Plus generic SQL means you get generic results which may require more logic or parsing | If you ‚Äúthink in SQL,‚Äù you will have to learn how to translate that into the functions that will generate that SQL | Talking in terms of application objects means that these objects, and how they map to tables, need to be defined up-front (and defined correctly) |

### A Word on Two Words: Configuration and Scope
Before we dive into the DAL examples, it‚Äôs worthwhile to talk through a few technical details that may arise as you advance further into this realm: configuration and scope.

#### Configuration
A major common element with the examples is that they rely on an _environment variable_, which we‚Äôre calling `DB_URL`, to specify how to connect to the underlying database server. This isn‚Äôt technically necessary for the sample programs but is a best practice in general‚Äînearly all real-world deployments need to keep this configuration piece separate from the code so that it can be changed easily without requiring a rebuild or recompile. Environment variables (worth looking up if you aren‚Äôt familiar with them) are a widely-used mechanism for this because they translate to a broad range of stacks and scenarios, ranging from the command line to so-called ‚Äúserverless‚Äù stacks like the AWS Lambda service. Getting a handle on them, especially on your friendly neighborhood command line, is a very useful and transferrable piece of knowledge.

A very straightforward‚Äîif verbose‚Äîway to pass environment variables to a program, _for that program call only_, is to supply them as key-value pairs before the command itself. Many command line shells support this; we use it here because it‚Äôs very clean, applying only to the program on that line and leaving no side effects:

    DB_URL=postgresql://localhost/postgres <rest of command>

As you get more comfortable, you can explore setting that variable for the duration of our session‚Ä¶ (_bash_/_zsh_ example shown)

    export DB_URL=postgresql://localhost/postgres

‚Ä¶to storing that line in an initialization file (this is command-line shell dependent), so that the variable is always set and would no longer need to be given with every command.

This last approach is very convenient but of course can leave state behind long after you no longer need it. To help with that, command-line shells provide a built-in command, such as `set`, `env`, or `setenv` (depends on the shell you‚Äôre using), that lists all currently active environment variables on a given command line session (you‚Äôd be surprised how many there are).

#### Scope
Multitiered/multilayered software architectures introduce new programming entities with more nuanced life cycles than local variables vs. global variables. There are _factories_ that create new objects for your DAL code, in a way that corresponds to current settings; _connections_ that represent active channels with the server; _transactions_ that determine when information is ‚Äúcommitted‚Äù permanently (or aborted); various _engines_ or _states_ that keep related information together, especially for ORM implementations.

Fully understanding the scopes and life cycles of these objects would be information overload right now, but suffice it to say that it‚Äôs good to at least know that such things exist and that using them correctly isn‚Äôt always solely a matter of ‚Äúmake it global‚Äù or ‚Äúmake it local.‚Äù There are layers in between those extremes whose value isn‚Äôt fully clear in these small examples, but which grow in prominence as you change architectures‚Äîfor example, when implementing a DAL for a web service. We‚Äôll seek to touch on these issues (especially transactions) when you‚Äôre more acclimated to all of this, but also stay open to learning more about them as you go, including beyond graduation and as new technologies emerge.

This also means that the DAL code given here isn‚Äôt necessarily set in stone if the DAL were to be used in other environments beyond single-use command-line programs. Ultimately, the sample code is meant to be a starting point for understanding how these systems work‚Äîthey seek to _illustrate_ the different aspects of this software stack without necessarily mandating particular ways for implementing them. If you decide to use this code as the basis for other projects, don‚Äôt be shocked if some rewriting becomes necessary. The hope is that, even through such rewrites, the roles and interactions of the sample code remain apparent even if the code evolves in its implementation or structure.

As for the DAL examples themselves, you‚Äôll find them in the [python](./python) and [nodejs](./nodejs) directories, with corresponding READMEs for getting them up and running. Enjoy!
